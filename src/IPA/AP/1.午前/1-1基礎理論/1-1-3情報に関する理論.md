# 1-1-3 情報に関する理論(译: 关于信息的理论)

- [1-1-3 情報に関する理論(译: 关于信息的理论)](#1-1-3-情報に関する理論译-关于信息的理论)
  - [情報量(译: 熵)](#情報量译-熵)
  - [ハフマン符号(译: 赫夫曼编码)](#ハフマン符号译-赫夫曼编码)
  - [オートマトン(译: 自动机)](#オートマトン译-自动机)
  - [木の走査順と逆ポーランド表記法(译: 二叉树遍历和逆波兰表示法)](#木の走査順と逆ポーランド表記法译-二叉树遍历和逆波兰表示法)
  - [BNF(Backus-Naur Form)記法(译: 巴科斯范式)](#bnfbackus-naur-form記法译-巴科斯范式)
  - [計算量(オーダ)(译: 时间复杂度(O))](#計算量オーダ译-时间复杂度o)
  - [AI(译: 人工知能)](#ai译-人工知能)
  - [機械学習(译: 机器学习)](#機械学習译-机器学习)
  - [ディープラーニング(译: 深度学习)](#ディープラーニング译-深度学习)

## 情報量(译: 熵)

- ある事象が起きたとき, それがどれくらい起こりにくいかを表す尺度である
  - 例: 6月に雨が降るのと雪が降るのとでは雪が降る方が起こりにくいので, 情報量としては多くなる
- ある事象が起こるときの情報量を, 選択情報量(自己エントロピー)という
  - $選択情報量 = - \log_{2}{P}$
    - Pはその事象が起こる確率である。
  - 例: 明日雨が降る確率が50%なら, 雨という事象の選択情報量を求めよ

    > $雨の選択情報量 = - \log_{2}{0.5} = - \log_{2}{2^{-1} = 1}$  
    > 選択情報量は1[ビット]となる

- 系全体(すべての場合を考えて), その全体の平均をとったときの情報量を平均情報量(エントロピー)という
  - $平均情報量 = {\textstyle \sum_{i = 1}^{n}} (選択情報量 \times P_i) = {\textstyle \sum_{i = 1}^{n}} \{ (- \log_{2}{P_i} \times P_i ) \}$
  - 例: 明日の天気が, 晴れが25%, 曇りが25%, 雨が50%なら, 平均情報量を求めよ

    > ① $晴れと曇りの選択情報量 = - \log_{2}{0.25} = - \log_{2}{2^{-2}} = 2$  
    > ② $雨の選択情報量 = - \log_{2}{0.5} = - \log_{2}{2^{-1}} = 1$  
    > ③ $平均情報量 = 2 \times 0.25 + 2 \times 0.25 + 1 \times 0.5 = 1.5$  
    > したがって, 平均情報量は1.5ビットである

- 例: a, b, c, dの4文字から成るメッセージを符号化してビット列にする方法として表のア ~ エの4通りを考えた。この表はa, b, c, dの各1文字を符号化するときのビット列を表している。メッセージ中でのa, b, c, dの出現頻度は, それぞれ50%, 30%, 10%, 10%であることが分かっている。符号化されたビット列から元のメッセージが一意に復号可能であって, ビット列の長さが最も短くなるものはどれ, か。

  | | a | b | c | d |
  | - | - | - | - | - |
  | ア | 0 | 1 | 00 | 11 |
  | イ | 0 | 01 | 10 | 11 |
  | ウ | 0 | 10 | 110 | 111 |
  | エ | 00 | 01 | 10 | 11 |

  > ① 符号化されたビット列から元のメッセージが一意に復号可能か  
  > ア: ビット列が[000]の場合, [aaa]か[ac]かが区別できないため, 不可  
  > イ: ビット列が[010]の場合, [ac]か[ba]かが区別できないため, 不可  
  > ウ: 元のメッセージを一意に識別することが可能  
  > エ: すべて2ビットなので, 2ビットごとに区切れば一意に復号可能  
  > ② ウ, エのビット列の長さを求める  
  > それぞれの[ビット長$\times$出現頻度]の合計値  
  > ウ: $1 \times 0.5 + 2 \times 0.3 + 3 \times 0.1 + 3 \times 0.1 = 1.7[ビット]$  
  > エ: $2 \times 0.5 + 2 \times 0.3 + 2 \times 0.1 + 2 \times 0.1 = 2[ビット]$  
  > したがって, 一意に復号可能かつビット列の長さが最も短くなるものはウである

- 上記例題から情報量について計算せよ

  > ① それぞれの選択情報量を算出する  
  > a, b, c, dの出現頻度はそれぞれ50%, 30%, 10%, 10%  
  > $aの選択情報量 = - \log_{2}{0.5} = 1$  
  > $bの選択情報量 = - \log_{2}{0.3} \approx 1.74$  
  > $c, dの選択情報量 = - \log_{2}{0.1} \approx 3.32$  
  > ② 平均情報量を算出する  
  > $平均情報量 = 1 \times 0.5 + 1.74 \times 0.3 + 3.32 \times 0.1 + 3.32 \times 0.1 = 1.686[ビット]$

  - この平均情報量1.686[ビット]というのは系全体がもっている情報量で, この値が圧縮の限界値にもなる

## ハフマン符号(译: 赫夫曼编码)

- エントロピー符号: それぞれの事象の出現確率をもとに事象ごとに異なる長さの符号を割り当てる考え方
  - 例: よく出現する文字には短いビット数を, あまり出現しない文字には長いビット数を割り当てることで, 全体のデータ量を削減する
- ハフマン符号
  - 1952年に米国のデビッド・ハフマンによって開発されたエントロピー符号の一つ
  - 2分木構造([データ構造](../1-2アルゴリズムとプログラミング/1-2-1.md)の一つ)を利用することで, 復号によって元の情報を復元できる可逆圧縮でありながらデータの全体量を減らすことができる方法
  - [参考](https://zhuanlan.zhihu.com/p/63362804)

## オートマトン(译: 自动机)

- オートマトン: 次のような三つの特徴をもったシステムのモデル
  - 外から, 情報が連続して入力される
  - 内部に, **状態**を保持する
  - 外へ, 情報を出力する
- 遷移: 特定の条件が起こったときに, ある状態から別の状態に移ること
  - 例: デートのとき, 相手が来るのを待っている状態が待ち状態で, 相手が来るという遷移条件が起こると, デートをするデート状態に移る
- 有限オートマトン: 状態や遷移の数を有限個で表すことができるオートマトン

## 木の走査順と逆ポーランド表記法(译: 二叉树遍历和逆波兰表示法)

- 走査: グラフ理論における木で, 木のそれぞれのノード(節点)を順番に読んでいくこと
- 走査の順番及びを表記する方法

  | 走査の順番 | 表記する方法 | 例(AとBの足し算) |
  | - | - | - |
  | 先行順 | 前置表記法(ポーランド表記法) | $+ A B$ |
  | 中間順 | 中置表記法 | $A + B$ |
  | 後行順 | 後置表記法(逆ポーランド表記法) | $A B +$ |

- 人間の頭脳が理解しやすいのは中置表記法, コンピュータは逆ポーランド表記法の方が処理しやすい
- 例: 式$A + B \times C$の逆ポーランド表記法による表現として, 適切なものはどれか。
  - ア: $+ \times C B A$
  - イ: $\times + A B C$
  - ウ: $A B C \times +$
  - エ: $C B A + \times$

  > 式$A + B \times C$では, $+$より$\times$の方が優先されるので, $A + (B \times C)$と考えることができる  
  > 木構造に直すと, 次のようになる  
  > $\quad +$  
  > $A \quad \times$  
  > $\quad B \quad C$  
  > この木を逆ポーランド表記法にするには後行順で順に読んでいく  
  > $A B C \times +$,  ウが正解である

## BNF(Backus-Naur Form)記法(译: 巴科斯范式)

- BNF記法: 文法などの形式を定義するために用いられる言語で, プログラム言語などの定義に利用される
- 特徴: 繰返しの表現に再帰を使う
- 例: あるプログラム言語において, 識別子(identifier)は, 先頭が英字で始まり, それ以降に任意個の英数字が続く文字列である。これをBNFで定義したとき, aに入るものはどれか。<br>$<digit> ::= 0|1|2|3|4|5|6|7|8|9$<br>$<letter> ::= A|B|C| \cdots |X|Y|Z|a|b|c| \cdots |x|y|z$<br>$<identifier> ::= a$
  - ア: $<letter>|<digit>|<identifier><letter>|<identifier><digit>$
  - イ: $<letter>|<digit>|<letter><identifier>|<identifier><digit>$
  - ウ: $<letter>|<identifier><digit>$
  - エ: $<letter>|<identifier><digit>|<identifier><letter>$

  > 記号の「$::=$」は「定義する」, 「$|$」は「いずれか」  
  > 識別子$<identifier>$: 先頭が英字で始まり, それ以降に任意個の英数字が続く文字列  
  > ① 1文字のみの場合:  
  > $<identifier> ::= <letter>$ $\cdots$英字のみ  
  > ② 2文字以上の場合:  
  > $<identifier> ::= <identifier><digit>|<identifier> ::= <identifier><letter>$ $\cdots$再帰を用いて, 2番目以降には英数字(英字か数字のどちらか)が続くという状態を表現できる  
  > したがって$<identifier> ::= <letter>|<identifier><digit>|<identifier><letter>$のようになり, エが正解である

## 計算量(オーダ)(译: 时间复杂度(O))

- 計算量: あるプログラム(アルゴリズム)を実行するのにどれくらいの時間がかかるかを, 入力データに対する増加量で表したもの
- **O-記法**: 計算量を表すときには, O-記法という表記法で, O(オーダ)という考え方が用いる
- 例: 入力データの数(n)が増加したとき, 計算量が
  - $n$に比例して増加する場合, $O(n)$と表す
  - $n^2$に比例して増加する場合, $O(n^2)$と表す
- 定数及び増加が小さい部分は無視する
  - 例: 計算量が$3n^2 + n + 2$に比例して増加する場合, 定数$3$及び増加が小さい部分$n + 2$が無視され, $O(n^2)$となる
- 代表的なO(オーダ)とその例

  | O(オーダ) | 例(アルゴリズム) |
  | - | - |
  | $O(1)$ | ハッシュ(c哈希) |
  | $O(\log n)$ | 2分探索(译: 二分查找) |
  | $O(n)$ | 線形探索(译: 线性查找) |
  | $O(n \log n)$ | クイックソート‌(译: 快速排序‌)<br>シェルソート(译: 希尔排序) |
  | $O(n^2)$ | バブルソート(译: 冒泡排序)<br>挿入ソート(译: 插入排序) |

## AI(译: 人工知能)

- AI: 人間と同様の知能をコンピュータ上で実現させるための技術
  - 強いAI: 人間を完全に模倣できる技術, まだ実現していない
  - 弱いAI: 人間の一部の機能を代替して実現できる, 実用化されている
- 代表的な実用化事例: 画像認識, 音声認識, テキスト翻訳

## 機械学習(译: 机器学习)

AIで利用する技術のうち, よく用いられる手法である。機械学習のアルゴリズムを使って, データの特性をコンピュータが自動的に学習する。

- 教師あり学習
  - 教師となる正解データ(ラベル)を用意する機械学習である。データを複数のグループに分ける**分類**や, 連続的なデータの値を予測する**回帰**を行うことができる。
  - 代表的なアルゴリズム:
    - サポートベクタマシン(译: 支持向量机)
      - 分類と回帰を行うことができるアルゴリズム
    - ニューラルネットワーク(译: 神经网络)
      - 人間の脳神経回路(ニューロン)を模倣して作成した, 分類と回帰を行うことができるアルゴリズム
      - ニューロンの階層を複数段重ねて複雑な学習を可能にしたアルゴリズムを**ディープラーニング(深層学習)**という
- 教師なし学習
  - データのみからその傾向を学習する手法である。データの性質で複数のグループに分けるクラスタリングなどを行うことができる。
  - 代表的なアルゴリズム:
    - K-means法(译: K均值(K-Means)聚类算法)
      - データをK個のクラスタにランダムに分け, そのクラスタごとの重心(平均となる座標)を求め, 今のクラスタよりも重心の距離が近いクラスタがあった場合に, クラスタを変更することを繰り返すアルゴリズム
- その他の機械学習
  - 代表的なアルゴリズム:
    - 強化学習(译: 强化学习)
      - 正解を用意するのではなく, 行動を試すための環境を用意し, とるべき行動を自分で学習していく方法
      - 半教師あり学習ともいえる

## ディープラーニング(译: 深度学习)

- ニューラルネットワークが発展してできたもの。大量のデータから精度の高いモデルを作成することができるため, 様々な分野で応用されている。
- ディープラーニングの技術を応用した実用的なアルゴリズム:
  - **CNN(Convolutional Neural Network: 畳み込みニューラルネットワーク)(译: 卷积神经网络)**
    - 画像解析などで主に活用されている
  - **RNN(Recurrent Neural Network: 再帰型ニューラルネットワーク)(译: 循环神经网络)**
    - 文章の翻訳や生成などの自然言語処理などで主に活用されている
- 例: AIにおけるディープラーニングに関する記述として, 最も適切なものはどれか。
  - ア: あるデータから結果を求める処理を, 人間の脳神経回路のように多層の処理を重ねることによって, 複雑な判断をできるようにする。
  - イ: 大量のデータからまだ知られていない新たな規則や仮説を発見するために, 想定値から大きく外れている例外事項を取り除きながら分析を繰り返す手法である。
  - ウ: 多様なデータや大量のデータに対して, 三段論法, 統計的手法やパターン認識手法を組み合わせることによって, 高度なデータ分析を行う手法である。
  - エ: 知識がルールに従って表現されており, 演繹手法を利用した推論によって有意な結論を導く手法である。

  > AIにおけるディープラーニングは, 機械学習のアルゴリズムの一つであるニューラルネットワークを多層化したものである。機械学習とは, データを学習して特定の結果を得ることで, ニューラルネットワークとは, 人間の脳神経回路をイメージした手法である。したがって, アが正解となる。
  > イ: データマイニング(译: 数据挖掘)に関する記述ある。
  > ウ: 一般的なデータ分析に関する記述ある。
  > エ: エキスパートシステム(译: 专家系统)など, ルールベースで行われる推論の手法ある。
